{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9766063-366b-454b-b224-014a8992f84a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Healthcare Payer Member Churn - Model Training & Comparison\n",
    "\n",
    "**Part 2 of 3**: This notebook covers model training, evaluation, and selection.\n",
    "\n",
    "## Prerequisites\n",
    "- Complete **Part 1: Feature Engineering** notebook first\n",
    "- Ensure the following tables exist:\n",
    "  - `demo.hls.label_features_versioned`\n",
    "  - `demo.hls.member_features_versioned`\n",
    "\n",
    "## What's Covered\n",
    "- Champion model training (Random Forest)\n",
    "- Model management and versioning\n",
    "- MLflow tracing\n",
    "- Challenger model training (Gradient Boosting)\n",
    "- Model comparison and selection\n",
    "- Hyperparameter tuning\n",
    "- Champion promotion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f694cfad-1242-44ca-a8ba-2fea6b72d828",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "# 4. Model Training with MLflow\n",
    "\n",
    "Now we'll train our member churn prediction model using MLflow for experiment tracking and model management.\n",
    "\n",
    "## 4.1 Create Training Dataset with Feature Lookups (Optional)\n",
    "\n",
    "Feature lookups allow you to automatically join features from the feature store at training and inference time. This example shows how to use them, though we'll use a simpler approach for this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52f78062-0cd2-4a1f-8268-59ed93ad5089",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define feature lookups if needed"
    }
   },
   "outputs": [],
   "source": [
    "# Example: How to define feature lookups (optional pattern)\n",
    "# This pattern is useful when you want to separate feature computation from model training\n",
    "from databricks.feature_store import FeatureFunction, FeatureLookup\n",
    "\n",
    "# Define feature specifications for runtime lookups\n",
    "features = [\n",
    "    FeatureLookup(\n",
    "        table_name=\"demo.hls.member_features_versioned\",\n",
    "        lookup_key=[\"member_id\"],\n",
    "        timestamp_lookup_key=\"version_ts\"  # Ensures point-in-time correctness\n",
    "    ),\n",
    "    FeatureFunction(\n",
    "        udf_name=\"demo.hls.current_age\",\n",
    "        input_bindings={\"dob\": \"dob\"},\n",
    "        output_name=\"age\"\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"✓ Feature specifications defined (for reference)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e1ed98a-88bc-4eab-ba11-3b82a5533a22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the labels and features for training\n",
    "labels_df = spark.read.table(\"demo.hls.label_features_versioned\")\n",
    "print(f\"Loaded {labels_df.count():,} records for training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "039d9dcc-7d80-4a55-aff7-fcc9c20b384e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create training set specs"
    }
   },
   "outputs": [],
   "source": [
    "# Create training set specifications using Feature Engineering Client\n",
    "# This links the features to the model for lineage tracking\n",
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "# Create training set with feature store linkage\n",
    "training_set_specs = fe.create_training_set(\n",
    "    df=labels_df,  # DataFrame with labels and features\n",
    "    label=\"disenrolled\",  # Target variable\n",
    "    feature_lookups=[],  # Empty list since features are already in the DataFrame\n",
    "    exclude_columns=[\"member_id\", \"version_ts\"]  # Exclude ID and timestamp from training\n",
    ")\n",
    "\n",
    "print(\"✓ Training set specifications created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9772d7f3-8994-4312-a665-3f9ca9a38125",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the training set as a Pandas DataFrame for scikit-learn training\n",
    "# training_set_specs.load_df() returns a PySpark DataFrame, which we convert to Pandas\n",
    "df_loaded = training_set_specs.load_df().toPandas()\n",
    "\n",
    "print(f\"✓ Training data loaded: {df_loaded.shape[0]:,} rows × {df_loaded.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be4deb2c-dbae-447d-83d0-083b16ed5556",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preview the training data\n",
    "df_loaded.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47fcd5f1-1855-4eec-ae89-66dfebfa7b1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## 4.2 Train Champion Model (Random Forest)\n",
    "\n",
    "We'll start by training a **Random Forest classifier** as our baseline (Champion) model. Random Forest is a good starting point because it:\n",
    "- Handles non-linear relationships well\n",
    "- Provides feature importance insights\n",
    "- Is robust to outliers and missing values\n",
    "- Works well out-of-the-box with minimal tuning\n",
    "\n",
    "**Data Splitting Strategy**:\n",
    "- **Training set (20%)**: For model training\n",
    "- **Validation set (40%)**: For hyperparameter tuning and model selection\n",
    "- **Test set (40%)**: For final evaluation (held out until the end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "106e6802-5df8-4e59-9231-f5b7bb1eabfb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Prepare Train, Val, Test Data"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare features and target variable\n",
    "# Exclude ID columns, timestamps, and the target variable from features\n",
    "selected_features = [\n",
    "    c for c in df_loaded.columns \n",
    "    if c not in [\"member_id\", \"version_ts\", \"last_claim_date\", \"first_claim_date\", \"disenrolled\", \"days_since_last_claim\"]\n",
    "]\n",
    "\n",
    "print(f\"Selected {len(selected_features)} features for modeling:\")\n",
    "print(selected_features)\n",
    "\n",
    "X = df_loaded[selected_features]  # Feature matrix\n",
    "y = df_loaded[\"disenrolled\"]  # Target variable\n",
    "\n",
    "# Split data: 20% train, 40% validation, 40% test\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(\n",
    "    X.fillna(0), y, \n",
    "    train_size=0.20, \n",
    "    random_state=42,\n",
    "    stratify=y  # Maintain class balance\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_rem, y_rem, \n",
    "    test_size=0.5, \n",
    "    random_state=42,\n",
    "    stratify=y_rem  # Maintain class balance\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Data split complete:\")\n",
    "print(f\"  Training set: {len(X_train):,} samples ({len(X_train)/len(X):.1%})\")\n",
    "print(f\"  Validation set: {len(X_val):,} samples ({len(X_val)/len(X):.1%})\")\n",
    "print(f\"  Test set: {len(X_test):,} samples ({len(X_test)/len(X):.1%})\")\n",
    "print(f\"  Disenrollment rate: {y.mean():.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "197e33ac-dac5-4ea7-a26e-338d02b9705a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Train RF model"
    }
   },
   "outputs": [],
   "source": [
    "# Train Random Forest model with MLflow tracking\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# End any existing run\n",
    "mlflow.end_run()\n",
    "\n",
    "# Start MLflow run with a descriptive name\n",
    "with mlflow.start_run(run_name=\"random_forest_champion\") as run:\n",
    "    # Set model hyperparameters\n",
    "    n_estimators = 100\n",
    "    max_depth = 6\n",
    "    random_state = 42\n",
    "    \n",
    "    # Log hyperparameters\n",
    "    mlflow.log_param(\"n_estimators\", n_estimators)\n",
    "    mlflow.log_param(\"max_depth\", max_depth)\n",
    "    mlflow.log_param(\"random_state\", random_state)\n",
    "    mlflow.log_param(\"algorithm\", \"RandomForest\")\n",
    "    \n",
    "    # Create and train model\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators, \n",
    "        max_depth=max_depth,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1  # Use all available cores\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on validation set\n",
    "    y_val_pred = rf.predict(X_val)\n",
    "    y_val_proba = rf.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Calculate and log metrics\n",
    "    val_roc_auc = roc_auc_score(y_val, y_val_proba)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    val_f1 = f1_score(y_val, y_val_pred)\n",
    "    val_precision = precision_score(y_val, y_val_pred)\n",
    "    val_recall = recall_score(y_val, y_val_pred)\n",
    "    \n",
    "    mlflow.log_metric(\"val_roc_auc\", val_roc_auc)\n",
    "    mlflow.log_metric(\"val_accuracy\", val_accuracy)\n",
    "    mlflow.log_metric(\"val_f1\", val_f1)\n",
    "    mlflow.log_metric(\"val_precision\", val_precision)\n",
    "    mlflow.log_metric(\"val_recall\", val_recall)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"RANDOM FOREST - VALIDATION METRICS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"ROC AUC:   {val_roc_auc:.4f}\")\n",
    "    print(f\"Accuracy:  {val_accuracy:.4f}\")\n",
    "    print(f\"F1 Score:  {val_f1:.4f}\")\n",
    "    print(f\"Precision: {val_precision:.4f}\")\n",
    "    print(f\"Recall:    {val_recall:.4f}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    run_id = run.info.run_id\n",
    "    print(f\"\\n✓ Model trained successfully (Run ID: {run_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78dd3d4f-611a-4019-a145-b84ddeaf0204",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Register Model to UC with feature engineering client"
    }
   },
   "source": [
    "---\n",
    "## 4.3 Register Model with Feature Store Lineage\n",
    "\n",
    "To maintain the connection between features and models, we use the **Feature Engineering client** to log the model. This enables:\n",
    "- **Automatic feature lookup** during inference\n",
    "- **Feature lineage tracking** (which features were used)\n",
    "- **Consistent feature computation** across training and serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccdca128-da0d-47b5-8ce2-f478b454209b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Register Model to UC with sklearn"
    }
   },
   "outputs": [],
   "source": [
    "# Create model signature and input example for documentation\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "input_example = X_train.iloc[[0]]\n",
    "signature = infer_signature(X_train, rf.predict(X_train))\n",
    "\n",
    "# Log model with Feature Store lineage using fe.log_model()\n",
    "# This maintains the connection between features and the model\n",
    "fe.log_model(\n",
    "    model=rf,\n",
    "    artifact_path=\"mx_churn\",\n",
    "    flavor=mlflow.sklearn,\n",
    "    training_set=training_set_specs,  # Links to feature store\n",
    "    input_example=input_example,\n",
    "    signature=signature,\n",
    "    registered_model_name=\"demo.hls.mx_churn\"\n",
    ")\n",
    "\n",
    "print(\"✓ Model registered in Unity Catalog with feature store lineage\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1a20201-9a2b-47f8-aab3-a33d3c950587",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## 4.4 Viewing Experiments and Models in Databricks\n",
    "\n",
    "### Viewing Experiment Runs:\n",
    "1. Click the **Experiments** icon in the right sidebar\n",
    "2. View parameters, metrics, and artifacts for each run\n",
    "3. Compare multiple runs side-by-side\n",
    "4. Click a run name to see detailed information\n",
    "\n",
    "### Viewing Registered Models:\n",
    "1. Navigate to **Catalog Explorer** (left sidebar → Catalog)\n",
    "2. Browse to `demo.hls.mx_churn`\n",
    "3. View all model versions, lineage, and metadata\n",
    "4. Track which features were used in each version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b841cd13-e577-413a-8bb4-ae9935502beb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Evaluate the Features"
    }
   },
   "source": [
    "---\n",
    "## 4.5 Feature Importance and Exploratory Analysis\n",
    "\n",
    "Understanding which features drive predictions helps:\n",
    "- **Build trust** in the model\n",
    "- **Identify intervention opportunities**\n",
    "- **Guide future feature engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a61eaec-7425-440d-a33f-58b59e1810bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Analyze feature importance from the Random Forest model\n",
    "importances = rf.feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    "    \"feature\": selected_features, \n",
    "    \"importance\": importances\n",
    "}).sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "# Plot top 15 most important features\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = importance_df.head(15)\n",
    "sns.barplot(data=top_features, x=\"importance\", y=\"feature\", hue=\"feature\", palette=\"viridis\", legend=False)\n",
    "plt.title(\"Top 15 Most Important Features for Churn Prediction\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Feature Importance\", fontsize=12)\n",
    "plt.ylabel(\"Feature\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(importance_df.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d999780-f1e1-4c44-8ed4-3263022d8a1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualize claim type distribution by disenrollment status\n",
    "claim_cols = [c for c in df_loaded.columns if c in [\"Outpatient\", \"Inpatient\", \"Pharmacy\", \"Professional\"]]\n",
    "if claim_cols:\n",
    "    claim_summary = df_loaded.groupby(\"disenrolled\")[claim_cols].mean().T\n",
    "    claim_summary.columns = [\"Retained\", \"Disenrolled\"]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    claim_summary.plot(kind=\"bar\", figsize=(10, 6), color=[\"#2ecc71\", \"#e74c3c\"])\n",
    "    plt.title(\"Average Claim Counts by Type and Disenrollment Status\", fontsize=14, fontweight='bold')\n",
    "    plt.ylabel(\"Average Claims per Member\", fontsize=12)\n",
    "    plt.xlabel(\"Claim Type\", fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title=\"Member Status\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nClaim Type Analysis:\")\n",
    "    print(claim_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae99bbb5-d2a8-4bc8-b6c0-f07a752dae90",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Tag Models and Register Champion Alias"
    }
   },
   "source": [
    "\n",
    "# 5. Model Management and Versioning\n",
    "\n",
    "Unity Catalog provides enterprise-grade model governance with:\n",
    "- **Version control**: Track all model versions\n",
    "- **Aliasing**: Use aliases like \"Champion\" and \"Challenger\" for A/B testing\n",
    "- **Lineage**: Trace features, data, and code used in each version\n",
    "- **Access control**: Manage who can view, use, or deploy models\n",
    "\n",
    "## 5.1 Load and Manage Model Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62bfe3f7-ca86-40d4-bf68-a0613f294586",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Initialize MLflow client"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize MLflow client for model management\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "model_name = \"demo.hls.mx_churn\"\n",
    "\n",
    "# Helper function to get the latest READY model version\n",
    "def get_latest_model_version(model_name):\n",
    "    \"\"\"Returns the latest READY version number for a model\"\"\"\n",
    "    model_version_infos = client.search_model_versions(f\"name = '{model_name}'\")\n",
    "    ready_versions = [\n",
    "        int(vars(v)[\"_version\"]) \n",
    "        for v in model_version_infos \n",
    "        if vars(v)[\"_status\"] == \"READY\"\n",
    "    ]\n",
    "    if not ready_versions:\n",
    "        raise ValueError(f\"No READY versions found for model {model_name}\")\n",
    "    return max(ready_versions)\n",
    "\n",
    "latest_version = get_latest_model_version(model_name)\n",
    "print(f\"✓ Latest model version: {latest_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef0fec5a-f9f1-4a4a-9c75-1e8e1a24df82",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Update model metadata and set aliases"
    }
   },
   "outputs": [],
   "source": [
    "# Update model metadata and set aliases\n",
    "# This provides context for model users and enables lifecycle management\n",
    "\n",
    "# Update overall model description\n",
    "client.update_registered_model(\n",
    "    name=model_name,\n",
    "    description=\"Healthcare member churn prediction model using claims data. Predicts disenrollment risk.\"\n",
    ")\n",
    "\n",
    "# Update specific version description\n",
    "client.update_model_version(\n",
    "    name=model_name,\n",
    "    version=latest_version,\n",
    "    description=\"Random Forest classifier (n_estimators=100, max_depth=6). Trained on member claims features.\"\n",
    ")\n",
    "\n",
    "# Set tags for tracking\n",
    "client.set_model_version_tag(\n",
    "    name=model_name,\n",
    "    version=str(latest_version),\n",
    "    key=\"stage\",\n",
    "    value=\"production\"\n",
    ")\n",
    "\n",
    "client.set_model_version_tag(\n",
    "    name=model_name,\n",
    "    version=str(latest_version),\n",
    "    key=\"algorithm\",\n",
    "    value=\"RandomForest\"\n",
    ")\n",
    "\n",
    "# Set \"Champion\" alias for this version\n",
    "# Aliases enable easy reference without hardcoding version numbers\n",
    "client.set_registered_model_alias(model_name, \"Champion\", latest_version)\n",
    "\n",
    "print(f\"✓ Model version {latest_version} set as 'Champion'\")\n",
    "print(f\"  Model URI: models:/{model_name}@Champion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1b8a4e5-2e3a-453e-bb0e-8426eed11db1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load and Use Model"
    }
   },
   "source": [
    "\n",
    "## 5.2 Viewing Models in Unity Catalog\n",
    "\n",
    "You can view and manage registered models in **Unity Catalog** using Catalog Explorer:\n",
    "\n",
    "1. In the left sidebar, click **Catalog**\n",
    "2. Navigate to `demo` → `hls` → `mx_churn`\n",
    "3. View:\n",
    "   - All model versions and their metadata\n",
    "   - Model lineage (features, datasets, code)\n",
    "   - Performance metrics\n",
    "   - Aliases and tags\n",
    "   - Access permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a1dc337-2d44-4b5f-8016-f1a92f473050",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Predict Functions"
    }
   },
   "source": [
    "\n",
    "## 5.3 Loading and Using Models\n",
    "\n",
    "Models can be loaded from Unity Catalog in multiple ways:\n",
    "- **By version number**: `models:/model_name/version`\n",
    "- **By alias**: `models:/model_name@alias`\n",
    "\n",
    "Using aliases (like \"Champion\") is recommended as it decouples code from specific versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cfbbec1-de32-4397-a0b7-8cc9b8d60c09",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Predictions"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "\n",
    "# Load model by version number\n",
    "latest_version = get_latest_model_version(model_name)\n",
    "model_version_uri = f\"models:/{model_name}/{latest_version}\"\n",
    "print(f\"Loading model from URI: {model_version_uri}\")\n",
    "model_by_version = mlflow.pyfunc.load_model(model_version_uri)\n",
    "\n",
    "# Load model by alias (recommended approach)\n",
    "model_champion_uri = f\"models:/{model_name}@Champion\"\n",
    "print(f\"Loading model from URI: {model_champion_uri}\")\n",
    "champion_model = mlflow.pyfunc.load_model(model_champion_uri)\n",
    "\n",
    "print(f\"\\n✓ Model loaded successfully: {champion_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f261530b-5961-4a2d-a24e-4379f5138896",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Helper functions for model predictions\n",
    "\n",
    "def load_and_predict(model_name, model_alias, new_data):\n",
    "    \"\"\"\n",
    "    Load model and make predictions (class labels)\n",
    "    Uses pyfunc loader which works for any MLflow model flavor\n",
    "    \"\"\"\n",
    "    model_uri = f\"models:/{model_name}@{model_alias}\"\n",
    "    model = mlflow.pyfunc.load_model(model_uri)\n",
    "    predictions = pd.DataFrame(model.predict(new_data))\n",
    "    return predictions\n",
    "\n",
    "def load_and_predict_proba(model_name, model_alias, new_data):\n",
    "    \"\"\"\n",
    "    Load sklearn model and get prediction probabilities\n",
    "    Uses sklearn loader to access predict_proba method\n",
    "    \"\"\"\n",
    "    model_uri = f\"models:/{model_name}@{model_alias}\"\n",
    "    model = mlflow.sklearn.load_model(model_uri)\n",
    "    probs = model.predict_proba(new_data)[:, 1]  # Probability of churn (class 1)\n",
    "    return probs\n",
    "\n",
    "print(\"✓ Prediction helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "54b65911-6c68-457e-b216-728228e561af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate predictions on validation set\n",
    "rf_pred = load_and_predict(model_name, \"Champion\", X_val)\n",
    "rf_pred.columns = [\"disenrollment_prediction\"]\n",
    "\n",
    "rf_prob = pd.DataFrame(load_and_predict_proba(model_name, \"Champion\", X_val))\n",
    "rf_prob.columns = [\"disenrollment_probability\"]\n",
    "\n",
    "# Combine features with predictions\n",
    "result_df = pd.concat([\n",
    "    X_val.reset_index(drop=True), \n",
    "    rf_pred.reset_index(drop=True), \n",
    "    rf_prob.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "print(f\"✓ Generated predictions for {len(result_df):,} members\")\n",
    "print(f\"  Average churn probability: {result_df['disenrollment_probability'].mean():.2%}\")\n",
    "print(f\"  Predicted churners: {result_df['disenrollment_prediction'].sum():,} ({result_df['disenrollment_prediction'].mean():.1%})\")\n",
    "\n",
    "display(result_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccf6b7f0-1f77-4f07-9ab6-e2ea47f3484b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Save Predictions"
    }
   },
   "outputs": [],
   "source": [
    "# Save predictions to Delta table for downstream use\n",
    "# Clean column names (replace spaces with underscores)\n",
    "result_df.columns = [col.replace(\" \", \"_\") for col in result_df.columns]\n",
    "\n",
    "spark.createDataFrame(result_df) \\\n",
    "    .write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(\"demo.hls.payer_1_disenroll_prediction\")\n",
    "\n",
    "print(\"✓ Predictions saved to demo.hls.payer_1_disenroll_prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b013d08-090a-4cb5-8c44-e306d31c7584",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 5.4 MLflow Tracing for Model Observability\n",
    "\n",
    "**MLflow Tracing** provides detailed visibility into model predictions:\n",
    "- Track inputs, outputs, and intermediate steps\n",
    "- Debug production issues\n",
    "- Monitor model behavior over time\n",
    "- Audit predictions for compliance\n",
    "\n",
    "Traces appear in the **Traces** tab of your experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "173426be-219c-460a-8960-893ba4e3935f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load and log model"
    }
   },
   "outputs": [],
   "source": [
    "# Load model and make predictions with tracing\n",
    "model_uri = f\"models:/{model_name}@Champion\"\n",
    "model = mlflow.sklearn.load_model(model_uri)\n",
    "\n",
    "# Use span to trace the prediction process\n",
    "with mlflow.start_span(name=\"predict_proba_with_trace\") as span:\n",
    "    # Log input metadata\n",
    "    span.set_inputs({\n",
    "        \"rows\": X_test.shape[0],\n",
    "        \"features\": list(X_test.columns),\n",
    "        \"model_alias\": \"Champion\"\n",
    "    })\n",
    "    \n",
    "    # Make predictions\n",
    "    proba = model.predict_proba(X_test)\n",
    "    pred = (proba[:, 1] >= 0.5).astype(int)\n",
    "    \n",
    "    # Log output samples (keep payloads small)\n",
    "    span.set_outputs({\n",
    "        \"proba_sample\": proba[:5].round(4).tolist(),\n",
    "        \"pred_sample\": pred[:10].tolist(),\n",
    "        \"churn_rate\": float(pred.mean())\n",
    "    })\n",
    "\n",
    "# Log metrics\n",
    "test_auc = roc_auc_score(y_test, proba[:, 1])\n",
    "test_accuracy = accuracy_score(y_test, pred)\n",
    "\n",
    "mlflow.log_metric(\"test_auc\", test_auc)\n",
    "mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CHAMPION MODEL - TEST SET METRICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ROC AUC:   {test_auc:.4f}\")\n",
    "print(f\"Accuracy:  {test_accuracy:.4f}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n✓ Prediction traced successfully (view in Experiments → Traces tab)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e88b6aa6-fc9f-4058-b4c2-85df8b36bbeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "# 6. Train Challenger Model (Gradient Boosting)\n",
    "\n",
    "To improve model performance, we'll train a **Gradient Boosting** classifier as a challenger model. \n",
    "\n",
    "**Champion/Challenger Pattern**:\n",
    "- **Champion**: Current production model (Random Forest)\n",
    "- **Challenger**: New model being evaluated (Gradient Boosting)\n",
    "- Compare performance before promoting the challenger\n",
    "\n",
    "Gradient Boosting often achieves better performance than Random Forest by:\n",
    "- Building trees sequentially to correct previous errors\n",
    "- Better handling of complex interactions\n",
    "- More accurate probability estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00e90904-d618-4e87-9c02-4be55757c36b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train Gradient Boosting classifier\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "mlflow.end_run()  # End any existing run\n",
    "\n",
    "with mlflow.start_run(run_name=\"gradient_boosting_challenger\") as run:\n",
    "    # Train with default parameters first\n",
    "    gb = GradientBoostingClassifier(random_state=42)\n",
    "    gb.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on validation set\n",
    "    y_val_pred = gb.predict(X_val)\n",
    "    y_val_proba = gb.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    val_roc_auc = roc_auc_score(y_val, y_val_proba)\n",
    "    val_f1 = f1_score(y_val, y_val_pred)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"val_roc_auc\", val_roc_auc)\n",
    "    mlflow.log_metric(\"val_f1\", val_f1)\n",
    "    mlflow.log_param(\"algorithm\", \"GradientBoosting\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"GRADIENT BOOSTING (Default) - VALIDATION METRICS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"ROC AUC:  {val_roc_auc:.4f}\")\n",
    "    print(f\"F1 Score: {val_f1:.4f}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    run_id = run.info.run_id\n",
    "    print(f\"\\n✓ Gradient Boosting model trained (Run ID: {run_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87e7c20c-c7c7-40b4-9d63-4a5d9b9b6753",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register Gradient Boosting model to Unity Catalog\n",
    "example_input = X_val.iloc[[0]]\n",
    "\n",
    "mlflow.sklearn.log_model(\n",
    "    sk_model=gb,\n",
    "    artifact_path=\"mx_churn\",\n",
    "    input_example=example_input,\n",
    "    registered_model_name=\"demo.hls.mx_churn\"\n",
    ")\n",
    "\n",
    "print(\"✓ Gradient Boosting model registered in Unity Catalog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4b37852-07cd-4a2b-8fbb-0696c44c60aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set this version as \"Challenger\" for A/B testing\n",
    "latest_version = get_latest_model_version(model_name)\n",
    "\n",
    "client.update_model_version(\n",
    "    name=model_name,\n",
    "    version=latest_version,\n",
    "    description=\"Gradient Boosting Classifier (default parameters). Challenger model for comparison.\"\n",
    ")\n",
    "\n",
    "client.set_model_version_tag(\n",
    "    name=model_name,\n",
    "    version=str(latest_version),\n",
    "    key=\"algorithm\",\n",
    "    value=\"GradientBoosting\"\n",
    ")\n",
    "\n",
    "client.set_registered_model_alias(model_name, \"Challenger\", latest_version)\n",
    "\n",
    "print(f\"✓ Model version {latest_version} set as 'Challenger'\")\n",
    "print(f\"  Model URI: models:/{model_name}@Challenger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fc1d961-c663-4ab8-9c95-7687137dd03e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "# 7. Model Comparison and Selection\n",
    "\n",
    "Before promoting a challenger to champion, we need rigorous comparison:\n",
    "- **Multiple metrics**: ROC AUC, F1, Precision, Recall, Brier Score\n",
    "- **Business context**: Which metric matters most for member retention?\n",
    "- **Validation set**: Used for model selection\n",
    "- **Test set**: Final unbiased evaluation (held out until the end)\n",
    "\n",
    "## 7.1 Define Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6137a791-f7c5-4136-8812-ea31f24eaea0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Comprehensive evaluation function\n",
    "def evaluate_model(y_true, y_probs, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate classification model with multiple metrics\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels\n",
    "        y_probs: Predicted probabilities\n",
    "        threshold: Classification threshold (default 0.5)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    y_pred = (y_probs >= threshold).astype(int)\n",
    "    \n",
    "    return {\n",
    "        \"ROC AUC\": roc_auc_score(y_true, y_probs),\n",
    "        \"F1\": f1_score(y_true, y_pred),\n",
    "        \"Precision\": precision_score(y_true, y_pred),\n",
    "        \"Recall\": recall_score(y_true, y_pred),\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Brier Score\": brier_score_loss(y_true, y_probs)\n",
    "    }\n",
    "\n",
    "print(\"✓ Evaluation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "442251d5-0244-40d8-a709-5a058e23e5ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compare Champion vs Challenger on validation set\n",
    "print(\"Loading models for comparison...\")\n",
    "rf_probs = load_and_predict_proba(model_name, \"Champion\", X_test)\n",
    "gb_probs = load_and_predict_proba(model_name, \"Challenger\", X_test)\n",
    "\n",
    "# Evaluate both models\n",
    "print(\"\\nEvaluating models...\")\n",
    "rf_results = evaluate_model(y_test, rf_probs)\n",
    "gb_results = evaluate_model(y_test, gb_probs)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison = pd.DataFrame(\n",
    "    [rf_results, gb_results], \n",
    "    index=[\"Random Forest (Champion)\", \"Gradient Boosting (Challenger)\"]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL COMPARISON - TEST SET RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison.to_string())\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Highlight improvements\n",
    "print(\"\\n\uD83D\uDCCA ANALYSIS:\")\n",
    "for metric in comparison.columns:\n",
    "    champion_val = rf_results[metric]\n",
    "    challenger_val = gb_results[metric]\n",
    "    \n",
    "    # For Brier Score, lower is better\n",
    "    if metric == \"Brier Score\":\n",
    "        improvement = ((champion_val - challenger_val) / champion_val) * 100\n",
    "        symbol = \"↓\" if challenger_val < champion_val else \"↑\"\n",
    "    else:\n",
    "        improvement = ((challenger_val - champion_val) / champion_val) * 100\n",
    "        symbol = \"↑\" if challenger_val > champion_val else \"↓\"\n",
    "    \n",
    "    print(f\"  {metric:12s}: {symbol} {improvement:+.2f}% change\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "130daa96-abe1-42b6-ac7c-c14faa2dcd31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## 7.2 Hyperparameter Tuning with Randomized Search\n",
    "\n",
    "To further improve the Gradient Boosting model, we'll use **Randomized Search CV** to find optimal hyperparameters:\n",
    "- More efficient than grid search for large parameter spaces\n",
    "- Tests random combinations of parameters\n",
    "- Uses cross-validation to avoid overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "792d38a9-fdcd-485a-afd4-cfaa8dfe837d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for Gradient Boosting\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Define hyperparameter search space\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(100, 1000),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'subsample': uniform(0.7, 0.3),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 5),\n",
    "}\n",
    "\n",
    "print(\"Starting hyperparameter search (this may take several minutes)...\")\n",
    "print(f\"Search space: {len(param_distributions)} parameters\")\n",
    "print(f\"Number of iterations: 50\")\n",
    "print(f\"Cross-validation folds: 5\\n\")\n",
    "\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "gb_search = RandomizedSearchCV(\n",
    "    estimator=gb,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,  # Number of parameter settings sampled\n",
    "    scoring='roc_auc',  # Optimize for ROC AUC\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "gb_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"HYPERPARAMETER TUNING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Best CV Score (ROC AUC): {gb_search.best_score_:.4f}\")\n",
    "print(f\"\\nBest Parameters:\")\n",
    "for param, value in gb_search.best_params_.items():\n",
    "    print(f\"  {param:20s}: {value}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6aeafdb-924c-401f-8502-07f4760795b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register the tuned Gradient Boosting model\n",
    "example_input = X_val.iloc[[0]]\n",
    "\n",
    "mlflow.sklearn.log_model(\n",
    "    sk_model=gb_search.best_estimator_,\n",
    "    artifact_path=\"mx_churn\",\n",
    "    input_example=example_input,\n",
    "    registered_model_name=\"demo.hls.mx_churn\"\n",
    ")\n",
    "\n",
    "print(\"✓ Tuned Gradient Boosting model registered in Unity Catalog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84bc787a-821d-40be-97f7-8eeccb9a63f2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Register best_estimator_ in Unity Catalog"
    }
   },
   "outputs": [],
   "source": [
    "# Update the Challenger alias to point to the tuned model\n",
    "latest_version = get_latest_model_version(model_name)\n",
    "\n",
    "client.update_model_version(\n",
    "    name=model_name,\n",
    "    version=latest_version,\n",
    "    description=\"Gradient Boosting with Randomized Search CV tuning. Optimized for ROC AUC.\"\n",
    ")\n",
    "\n",
    "client.set_model_version_tag(\n",
    "    name=model_name,\n",
    "    version=str(latest_version),\n",
    "    key=\"algorithm\",\n",
    "    value=\"GradientBoosting_Tuned\"\n",
    ")\n",
    "\n",
    "client.set_registered_model_alias(model_name, \"Challenger\", latest_version)\n",
    "\n",
    "print(f\"✓ Tuned model (version {latest_version}) set as 'Challenger'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17db0a98-0794-49f1-926b-6e890786fc00",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Label Model as New Challenger"
    }
   },
   "outputs": [],
   "source": [
    "# Final comparison: Champion vs Tuned Challenger\n",
    "print(\"Evaluating tuned challenger model...\\n\")\n",
    "\n",
    "gb_tuned_probs = load_and_predict_proba(model_name, \"Challenger\", X_test)\n",
    "rf_probs = load_and_predict_proba(model_name, \"Champion\", X_test)\n",
    "\n",
    "# Evaluate both models\n",
    "gb_tuned_results = evaluate_model(y_test, gb_tuned_probs)\n",
    "rf_results = evaluate_model(y_test, rf_probs)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "final_comparison = pd.DataFrame(\n",
    "    [rf_results, gb_tuned_results], \n",
    "    index=[\"Random Forest (Champion)\", \"Gradient Boosting - Tuned (Challenger)\"]\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FINAL MODEL COMPARISON - TEST SET RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(final_comparison.to_string())\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Decision logic\n",
    "print(\"\\n\uD83C\uDFAF RECOMMENDATION:\")\n",
    "if gb_tuned_results[\"ROC AUC\"] > rf_results[\"ROC AUC\"]:\n",
    "    improvement = ((gb_tuned_results[\"ROC AUC\"] - rf_results[\"ROC AUC\"]) / rf_results[\"ROC AUC\"]) * 100\n",
    "    print(f\"✅ PROMOTE Challenger to Champion\")\n",
    "    print(f\"   ROC AUC improved by {improvement:.2f}%\")\n",
    "else:\n",
    "    print(f\"⏸️  KEEP current Champion\")\n",
    "    print(f\"   Challenger did not outperform Champion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cabb4ce-5eed-480b-a11e-728ddde9a08a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Compare Models"
    }
   },
   "source": [
    "---\n",
    "## 7.3 Promote Challenger to Champion\n",
    "\n",
    "Once we've validated that the challenger outperforms the champion, we can promote it. This involves:\n",
    "1. Updating the \"Champion\" alias to point to the new model\n",
    "2. Optionally archiving or removing the \"Challenger\" alias\n",
    "3. Updating model tags and documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0defab4-26e4-421e-a881-5d746d792a5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Promote the Challenger to Champion\n",
    "new_champion_version = get_latest_model_version(model_name)\n",
    "\n",
    "# Update Champion alias to point to the new best model\n",
    "client.set_registered_model_alias(\n",
    "    name=model_name,\n",
    "    alias=\"Champion\",\n",
    "    version=new_champion_version\n",
    ")\n",
    "\n",
    "# Update tags\n",
    "client.set_model_version_tag(\n",
    "    name=model_name,\n",
    "    version=str(new_champion_version),\n",
    "    key=\"stage\",\n",
    "    value=\"production\"\n",
    ")\n",
    "\n",
    "# Remove Challenger alias (optional)\n",
    "try:\n",
    "    client.delete_registered_model_alias(name=model_name, alias=\"Challenger\")\n",
    "    print(\"✓ Removed 'Challenger' alias\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(f\"\\n\uD83C\uDF89 Model version {new_champion_version} promoted to Champion!\")\n",
    "print(f\"   URI: models:/{model_name}@Champion\")\n",
    "print(f\"\\n\uD83D\uDCDD Next steps:\")\n",
    "print(f\"   1. Deploy to production endpoint\")\n",
    "print(f\"   2. Monitor performance metrics\")\n",
    "print(f\"   3. Set up alerts for model drift\")\n",
    "\n",
    "# Note: Uncomment the following lines to delete old model versions if needed\n",
    "# client.delete_model_version(name=model_name, version=old_version)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_41a50460-c90b-4840-9288-afcb847395d5",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8994656465154948,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "2_Model_Training_and_Comparison",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}