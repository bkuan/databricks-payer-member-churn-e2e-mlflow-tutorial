{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Healthcare Payer Member Churn - Deployment & Monitoring\n",
    "\n",
    "**Part 3 of 3**: This notebook covers model deployment and production operations.\n",
    "\n",
    "## Prerequisites\n",
    "- Complete **Part 1: Feature Engineering** notebook\n",
    "- Complete **Part 2: Model Training & Comparison** notebook\n",
    "- Ensure you have a trained model registered as:\n",
    "  - `demo.hls.mx_churn@Champion`\n",
    "\n",
    "## What's Covered\n",
    "- Batch inference\n",
    "- Real-time model serving\n",
    "- SQL-based inference\n",
    "- Model monitoring strategies\n",
    "- Databricks SQL dashboards\n",
    "- Business actions and interventions\n",
    "- Production roadmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ead66e5-0d66-43e3-91b4-e1458f262726",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "---\n",
    "# 8. Model Deployment Options\n",
    "\n",
    "Databricks provides multiple deployment options for ML models:\n",
    "\n",
    "1. **Batch Inference**: Score large datasets using Spark (covered below)\n",
    "2. **Model Serving**: Real-time REST API endpoints\n",
    "3. **Streaming**: Process data streams with Structured Streaming\n",
    "4. **Embedded**: Export models for edge deployment\n",
    "\n",
    "## 8.1 Batch Inference with Feature Engineering\n",
    "\n",
    "Batch scoring is ideal for:\n",
    "- Periodic member risk assessments\n",
    "- Bulk predictions on enrollment data\n",
    "- Offline model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01a8baff-aeb2-4b93-87b6-4ba5fa5ce517",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Batch inference using Feature Engineering Client\n",
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Load members to score (in production, this would be your active member base)\n",
    "inference_df = spark.read.table(\"demo.hls.label_features_versioned\").drop(\"disenrolled\")\n",
    "\n",
    "print(f\"Scoring {inference_df.count():,} members...\")\n",
    "\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "# Model URI with Champion alias\n",
    "model_uri = f\"models:/{model_name}@Champion\"\n",
    "\n",
    "# Batch score with feature engineering\n",
    "# This automatically joins features from the feature store\n",
    "predictions_df = fe.score_batch(\n",
    "    df=inference_df,\n",
    "    model_uri=model_uri,\n",
    "    result_type=DoubleType()  # Returns probability scores\n",
    ")\n",
    "\n",
    "print(f\"\\n\u2713 Batch scoring complete\")\n",
    "print(f\"\\nPreview of predictions:\")\n",
    "display(predictions_df.limit(100))\n",
    "\n",
    "# Save predictions for downstream use\n",
    "predictions_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"demo.hls.member_churn_scores\")\n",
    "\n",
    "print(\"\\n\u2713 Predictions saved to demo.hls.member_churn_scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1fa685b-3d70-43b3-bcd4-7a5146395be0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dfd94c1-fd9f-4ee9-bdc0-ab0a927d3ecf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "---\n",
    "## 8.2 Real-Time Model Serving\n",
    "\n",
    "**Model Serving** provides production-ready REST API endpoints for real-time predictions:\n",
    "\n",
    "### Benefits:\n",
    "- **Low latency**: Sub-second response times\n",
    "- **Auto-scaling**: Handles variable load\n",
    "- **Monitoring**: Built-in metrics and logging\n",
    "- **A/B testing**: Route traffic between model versions\n",
    "- **Authentication**: Secure with tokens\n",
    "\n",
    "### Setup Options:\n",
    "1. **UI**: Navigate to Serving \u2192 Create Serving Endpoint\n",
    "2. **API**: Programmatically via REST API (shown below)\n",
    "3. **Terraform**: Infrastructure as code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ddf643d-d49d-439e-af54-ab0a8dd9113a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model Serving Setup\n",
    "# Note: This is informational - actual endpoint creation is typically done via UI or API\n",
    "\n",
    "print(\"\ud83d\udccd To create a Model Serving endpoint:\\n\")\n",
    "print(\"1. Navigate to the Databricks workspace\")\n",
    "print(\"2. Click 'Serving' in the left sidebar\")\n",
    "print(\"3. Click 'Create Serving Endpoint'\")\n",
    "print(\"4. Select model: demo.hls.mx_churn@Champion\")\n",
    "print(\"5. Configure:\")\n",
    "print(\"   - Workload size: Small (for testing)\")\n",
    "print(\"   - Scale to zero: Enabled (cost optimization)\")\n",
    "print(\"6. Click 'Create'\\n\")\n",
    "\n",
    "print(\"\ud83d\udccd Alternative: Query using AI_QUERY function (shown below)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fc2b9b5-62f7-413a-85e8-1e3077fd938a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%sql\n",
    "-- Query model endpoint using AI_QUERY function\n",
    "-- This requires a model serving endpoint named 'mx_churn' to be created first\n",
    "\n",
    "-- Example query:\n",
    "SELECT ai_query(\n",
    "  'mx_churn',  -- endpoint name\n",
    "  request => named_struct(\n",
    "    'num_claims_total', 15,\n",
    "    'num_distinct_providers', 3,\n",
    "    'num_months_with_claims', 8,\n",
    "    'pct_denied_claims', 0.10,\n",
    "    'total_charge_amount', 25000.0,\n",
    "    'total_paid_amount', 20000.0,\n",
    "    'avg_allowed_amount', 1500.0,\n",
    "    'avg_paid_amount', 1300.0,\n",
    "    'age', 45.0,\n",
    "    'avg_days_between_claims', 30.0,\n",
    "    'Inpatient', 2,\n",
    "    'Outpatient', 8,\n",
    "    'Pharmacy', 15,\n",
    "    'Professional', 10,\n",
    "    'avg_allowed_paid_ratio', 1.15\n",
    "  )\n",
    ") AS churn_probability;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "497bdb8f-ae37-4f39-a16b-bff3b18fab49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Example JSON payload for REST API call\n",
    "example_payload = {\n",
    "    \"num_claims_total\": 15,\n",
    "    \"num_distinct_providers\": 3,\n",
    "    \"num_months_with_claims\": 8,\n",
    "    \"pct_denied_claims\": 0.10,\n",
    "    \"total_charge_amount\": 25000.0,\n",
    "    \"total_paid_amount\": 20000.0,\n",
    "    \"avg_allowed_amount\": 1500.0,\n",
    "    \"avg_paid_amount\": 1300.0,\n",
    "    \"age\": 45.0,\n",
    "    \"avg_days_between_claims\": 30.0,\n",
    "    \"Inpatient\": 2,\n",
    "    \"Outpatient\": 8,\n",
    "    \"Pharmacy\": 15,\n",
    "    \"Professional\": 10,\n",
    "    \"avg_allowed_paid_ratio\": 1.15\n",
    "}\n",
    "\n",
    "import json\n",
    "print(\"Example REST API payload:\")\n",
    "print(json.dumps(example_payload, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f847b1ac-2aa2-4eba-ac1c-fb1aad81a89e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "---\n",
    "# 9. Model Monitoring and Operations\n",
    "\n",
    "After deployment, continuous monitoring is essential for production ML systems.\n",
    "\n",
    "## 9.1 Key Monitoring Areas\n",
    "\n",
    "### Model Performance:\n",
    "- **Prediction accuracy**: Track precision, recall, F1 over time\n",
    "- **Business metrics**: Actual vs predicted churn rates\n",
    "- **Confusion matrix**: False positives vs false negatives\n",
    "\n",
    "### Data Quality:\n",
    "- **Feature drift**: Are input distributions changing?\n",
    "- **Missing values**: Track NULL rates for key features\n",
    "- **Outliers**: Detect anomalous input values\n",
    "\n",
    "### System Health:\n",
    "- **Latency**: Response time percentiles (p50, p95, p99)\n",
    "- **Throughput**: Requests per second\n",
    "- **Error rates**: Failed predictions\n",
    "- **Resource usage**: CPU, memory, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08c1b4a9-d26e-4e26-aeb2-b4e2750265b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "---\n",
    "## 9.2 Databricks SQL Dashboards\n",
    "\n",
    "Create operational dashboards to monitor model performance:\n",
    "\n",
    "### Dashboard Components:\n",
    "\n",
    "**1. Churn Risk Distribution**\n",
    "```sql\n",
    "SELECT \n",
    "  CASE \n",
    "    WHEN prediction > 0.7 THEN 'High Risk'\n",
    "    WHEN prediction > 0.3 THEN 'Medium Risk'\n",
    "    ELSE 'Low Risk'\n",
    "  END as risk_category,\n",
    "  COUNT(*) as member_count\n",
    "FROM demo.hls.member_churn_scores\n",
    "GROUP BY risk_category\n",
    "```\n",
    "\n",
    "**2. Top Risk Members**\n",
    "```sql\n",
    "SELECT \n",
    "  member_id,\n",
    "  prediction as churn_probability,\n",
    "  num_claims_total,\n",
    "  age\n",
    "FROM demo.hls.member_churn_scores\n",
    "ORDER BY prediction DESC\n",
    "LIMIT 100\n",
    "```\n",
    "\n",
    "**3. Feature Importance Trends**\n",
    "- Track which features drive predictions over time\n",
    "- Identify changing patterns in member behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63c4bcd6-21b5-4c6e-8227-053143b100a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## 9.3 Databricks Lakehouse Monitoring\n",
    "\n",
    "**Lakehouse Monitoring** (formerly known as Model Monitoring) provides automated drift detection:\n",
    "\n",
    "### Setup:\n",
    "```python\n",
    "# Example: Create a monitor for your predictions table\n",
    "import databricks.lakehouse_monitoring as lm\n",
    "\n",
    "lm.create_monitor(\n",
    "    table_name=\"demo.hls.member_churn_scores\",\n",
    "    granularities=[\"1 day\"],  # Monitoring frequency\n",
    "    output_schema_name=\"demo.hls_monitoring\",\n",
    "    baseline_table_name=\"demo.hls.member_churn_baseline\"  # Historical baseline\n",
    ")\n",
    "```\n",
    "\n",
    "### Monitors Track:\n",
    "- **Statistical drift**: Distribution changes in features\n",
    "- **Prediction drift**: Changes in model output distribution\n",
    "- **Label drift**: If actual labels become available\n",
    "- **Data quality**: Missing values, schema changes\n",
    "\n",
    "### Alerts:\n",
    "- Configure email/Slack notifications\n",
    "- Set thresholds for acceptable drift\n",
    "- Trigger retraining pipelines automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a853c628-6bc3-400d-ad86-b4696a7c96a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## 9.4 Databricks Genie (Conversational AI)\n",
    "\n",
    "**Databricks Genie** enables natural language interaction with your churn predictions:\n",
    "\n",
    "### Example Questions:\n",
    "- \"Show me members with high churn risk in California\"\n",
    "- \"What's the average churn probability by age group?\"\n",
    "- \"Which members have the highest predicted churn and low engagement?\"\n",
    "- \"Compare churn rates between pharmacy and medical users\"\n",
    "\n",
    "### Business Value:\n",
    "- **Democratize insights**: Non-technical users can query predictions\n",
    "- **Faster decisions**: Natural language vs writing SQL\n",
    "- **Exploratory analysis**: Quickly test hypotheses\n",
    "\n",
    "### Setup:\n",
    "1. Navigate to your workspace\n",
    "2. Click on **Genie** in the left sidebar\n",
    "3. Connect to your schema (demo.hls)\n",
    "4. Start asking questions!\n",
    "\n",
    "Genie can automatically:\n",
    "- Generate appropriate SQL queries\n",
    "- Create visualizations\n",
    "- Provide insights and explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53525dcf-55a9-41e4-8cb6-979c5bb2b0b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "# 10. Business Actions and Interventions\n",
    "\n",
    "The ultimate goal is to use predictions to **reduce member churn**. Here's how:\n",
    "\n",
    "## 10.1 Risk-Based Interventions\n",
    "\n",
    "### High Risk Members (probability > 0.7):\n",
    "- **Personal outreach**: Dedicated care coordinator contact\n",
    "- **Retention offers**: Premium waivers, enhanced benefits\n",
    "- **Root cause analysis**: Why are they considering leaving?\n",
    "- **Expedited issue resolution**: Fast-track any complaints\n",
    "\n",
    "### Medium Risk Members (0.3 - 0.7):\n",
    "- **Automated engagement**: Email campaigns highlighting benefits\n",
    "- **Education**: Explain unused benefits and services\n",
    "- **Preventive care nudges**: Wellness checkups, screenings\n",
    "- **Satisfaction surveys**: Proactive feedback collection\n",
    "\n",
    "### Low Risk Members (< 0.3):\n",
    "- **Standard engagement**: Regular newsletters\n",
    "- **Advocacy programs**: Turn satisfied members into advocates\n",
    "- **Upsell opportunities**: Additional coverage options\n",
    "\n",
    "## 10.2 Measuring Impact\n",
    "\n",
    "Track the effectiveness of interventions:\n",
    "- **Retention rate**: % of at-risk members who stayed\n",
    "- **Cost per retention**: Intervention cost vs lifetime value\n",
    "- **Time to churn**: Did interventions delay disenrollment?\n",
    "- **ROI**: Value saved from reduced churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9941810-b8e5-46ce-95b2-455fd06eaa68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "# 11. Summary and Next Steps\n",
    "\n",
    "## \u2705 What We've Built\n",
    "\n",
    "This end-to-end ML solution demonstrates:\n",
    "\n",
    "1. **Feature Engineering**: Scalable feature pipelines with Databricks Feature Store\n",
    "2. **Model Training**: Multiple algorithms with MLflow tracking\n",
    "3. **Model Comparison**: Rigorous evaluation with Champion/Challenger pattern\n",
    "4. **Model Registry**: Centralized governance with Unity Catalog\n",
    "5. **Deployment**: Batch and real-time inference options\n",
    "6. **Monitoring**: Comprehensive observability and drift detection\n",
    "\n",
    "## \ud83d\ude80 Recommended Next Steps\n",
    "\n",
    "### Short Term (1-2 weeks):\n",
    "1. **Deploy to production**: Set up model serving endpoint\n",
    "2. **Schedule batch scoring**: Daily predictions for all active members\n",
    "3. **Create dashboards**: Visualize churn risk trends\n",
    "4. **Integrate with CRM**: Send high-risk members to retention team\n",
    "\n",
    "### Medium Term (1-2 months):\n",
    "1. **A/B test interventions**: Measure impact of retention programs\n",
    "2. **Add more features**: Social determinants, claims patterns, satisfaction scores\n",
    "3. **Segment models**: Build specialized models by plan type or region\n",
    "4. **Explainability**: Add SHAP values to explain individual predictions\n",
    "\n",
    "### Long Term (3-6 months):\n",
    "1. **AutoML integration**: Automate model selection and hyperparameter tuning\n",
    "2. **Real-time features**: Streaming feature computation\n",
    "3. **Multi-model ensemble**: Combine multiple algorithms\n",
    "4. **Causal inference**: Move beyond prediction to understand causation\n",
    "5. **Prescriptive analytics**: Recommend optimal intervention strategies\n",
    "\n",
    "## \ud83d\udcda Additional Resources\n",
    "\n",
    "- [Databricks MLflow Documentation](https://docs.databricks.com/mlflow/index.html)\n",
    "- [Feature Engineering in Unity Catalog](https://docs.databricks.com/machine-learning/feature-store/index.html)\n",
    "- [Model Serving Guide](https://docs.databricks.com/machine-learning/model-serving/index.html)\n",
    "- [Lakehouse Monitoring](https://docs.databricks.com/lakehouse-monitoring/index.html)\n",
    "\n",
    "## \ud83d\udca1 Key Takeaways\n",
    "\n",
    "1. **MLflow + Unity Catalog** = Complete model lifecycle management\n",
    "2. **Feature Store** ensures consistency between training and serving\n",
    "3. **Champion/Challenger** pattern enables safe model updates\n",
    "4. **Monitoring** is essential for production ML success\n",
    "5. **Business impact** is the ultimate measure of success\n",
    "\n",
    "---\n",
    "\n",
    "**Questions or feedback?** This notebook was adapted from the [Databricks MLflow End-to-End Example](https://docs.databricks.com/aws/en/notebooks/source/mlflow/mlflow-classic-ml-e2e-mlflow-3.html) for healthcare payer use cases."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_41a50460-c90b-4840-9288-afcb847395d5",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8994656465154948,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Healthcare Payer Member Churn - Model Development",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}